import sysfrom awsglue.transforms import *from awsglue.utils import getResolvedOptionsfrom pyspark.context import SparkContextfrom awsglue.context import GlueContextfrom awsglue.job import Job## @params: [JOB_NAME]args = getResolvedOptions(sys.argv, ['JOB_NAME'])sc = SparkContext()glueContext = GlueContext(sc)spark = glueContext.spark_sessionjob = Job(glueContext)job.init(args['JOB_NAME'], args)S3bucket_node1 = glueContext.create_dynamic_frame.from_catalog(    database="fakedbstream", table_name="dataeng_test_landing1_2025", transformation_ctx="S3bucket_node1"    )S3bucket_node3 = glueContext.write_dynamic_frame.from_options(    frame=S3bucket_node1,    connection_type="s3",    format="glueparquet",    connection_options={"path": "s3://dataeng-test-curated-2025", "partitionKeys": []},    format_options={"compression": "gzip"},    transformation_ctx="S3bucket_node3",    )job.commit()